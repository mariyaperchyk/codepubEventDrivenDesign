{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Event Driven Design - Music Streaming App-Task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNwjztABceJMmFeaVvJknuE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariyaperchyk/codepubEventDrivenDesign/blob/main/Event_Driven_Design_Music_Streaming_App_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erU8e-mbGfko"
      },
      "source": [
        "# Event Driven Design - Music Streaming App\n",
        "\n",
        "This notebook demonstrates how to setup a kafka cluster (kafka and zookeeper) and implements a producer and a consumer to simulate a music streaming application. \n",
        "The producer generates a stream of plays (based on a csv file) and writes these plays to a kafka topic. \n",
        "The consumer listens to the topic and aggregates the plays for analytics, e.g., which genre is played most often. \n",
        "\n",
        "A few lines of code have been left out for you to fill!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOyf-EtdH_hw"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63gniAmMF8Iz"
      },
      "source": [
        "## Download and setup Kafka and Zookeeper instances\n",
        "For demo purposes, the following instances are setup locally:\n",
        "\n",
        "- Kafka (Brokers: 127.0.0.1:9092) Kafka Broker is a server that runs Kafka. It stored the data and handles requests from clients/applications. \n",
        "\n",
        "- Zookeeper (Node: 127.0.0.1:2181) Zookeeper keeps the state of the kafka cluster, e.g. data such as the location of partitions and the configuration of topics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YinBwgrAFPa0"
      },
      "source": [
        "!curl -sSOL https://downloads.apache.org/kafka/2.7.0/kafka_2.13-2.7.0.tgz\n",
        "!tar -xzf kafka_2.13-2.7.0.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiKRa_V9GHo-"
      },
      "source": [
        "Using the default configurations (provided by Apache Kafka), we spin up one kafka broker and one zookeeper. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfMdsA7ZFh-b"
      },
      "source": [
        "!./kafka_2.13-2.7.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-2.7.0/config/zookeeper.properties\n",
        "!./kafka_2.13-2.7.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-2.7.0/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyonH5E2GSf8"
      },
      "source": [
        "Once the instances are started as daemon processes, grep for `kafka` in the processes list. The two java processes correspond to our zookeeper and the kafka instances. Make suyre you have both processes running. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbwLjtLAFmLS"
      },
      "source": [
        "!ps -ef | grep kafka"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH-bv7RhGW-3"
      },
      "source": [
        "## Create Kafka topic\n",
        "\n",
        "We can now create one kafka topic with the following specs:\n",
        "\n",
        "- music-streams: partitions=2, replication-factor=1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXyEhgK0FqdU"
      },
      "source": [
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic music-streams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4JUo7gfGdrP"
      },
      "source": [
        "Let's take a look at the topic that we created. We can describe it to get configuration details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZo6SMQSFwIU"
      },
      "source": [
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic music-streams\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNpTnPdyGg5d"
      },
      "source": [
        "## Install required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oyQDCgQG37S"
      },
      "source": [
        "!pip install confluent-kafka"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q0dWrSmHndM"
      },
      "source": [
        "# Implement Producer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n76iu9x2YQQP"
      },
      "source": [
        "Download our dataset of music streams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DfXc_fEXLou"
      },
      "source": [
        "!curl -o music-streams.csv -s https://owncloud.hpi.de/s/bDHMeaX3029rT2E/download\n",
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdyDyWt90lef"
      },
      "source": [
        "Let's take a look into the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v-724tu0q1R"
      },
      "source": [
        "!head music-streams.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-1m9CBPYXhB"
      },
      "source": [
        "`music-streams.csv` simulates a music streaming application. Each row in the file represents a play of a song. Here, a play consists of a title, an artist, a genre and year in which the song was produced. Same songs might appear in the file multiple times, as a user might listen to the same song again and again. \n",
        "\n",
        "Now its time to implement the producer. The producer should read the plays from the `music-streams.csv` file and produce play events, which are written to the `music-streams` topic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Cm2WFdHqwL"
      },
      "source": [
        "import csv\n",
        "from confluent_kafka import SerializingProducer\n",
        "from confluent_kafka.serialization import StringSerializer\n",
        "\n",
        "def get_producer():\n",
        "    producer_conf = {'bootstrap.servers': '127.0.0.1:9092',\n",
        "                     'key.serializer': StringSerializer('utf_8'),\n",
        "                     'value.serializer': StringSerializer('utf_8')}\n",
        "    return SerializingProducer(producer_conf)\n",
        "\n",
        "def write_to_kafka(topic_name, file):\n",
        "  count=0\n",
        "  producer = get_producer()\n",
        "  with open(file) as csvfile:\n",
        "    reader = csv.DictReader(csvfile, delimiter=';')\n",
        "\n",
        "    for row in reader:\n",
        "      #Retrieve delivery reports, this ensures that the write queue does not grow indefinitely\n",
        "      producer.poll(0.0)\n",
        "      producer.produce(\n",
        "          topic=topic_name, \n",
        "          key=row['Top Genre'], \n",
        "          # Note: we are writing an OrderedDict as the value here\n",
        "          value=str(row))\n",
        "      count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "write_to_kafka('music-streams', 'music-streams.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVg_FAGb4PMN"
      },
      "source": [
        "# Implement Consumer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59iM76294RF4"
      },
      "source": [
        "from confluent_kafka import DeserializingConsumer\n",
        "from confluent_kafka.serialization import StringDeserializer\n",
        "from collections import defaultdict, OrderedDict\n",
        "\n",
        "\n",
        "def get_consumer(topic_name, consumer_group):\n",
        "  consumer_conf = {'bootstrap.servers': '127.0.0.1:9092',\n",
        "                     'key.deserializer': StringDeserializer('utf_8'),\n",
        "                     'value.deserializer': StringDeserializer('utf_8'),\n",
        "                     'group.id': consumer_group,\n",
        "                     'auto.offset.reset': \"earliest\"}\n",
        "\n",
        "  consumer = DeserializingConsumer(consumer_conf)\n",
        "  consumer.subscribe([topic_name])\n",
        "  return consumer\n",
        "\n",
        "def get_artist_statistics(topic_name, consumer_group):\n",
        "  consumer = get_consumer(topic_name, consumer_group)\n",
        "  artist_statistics = defaultdict(int)\n",
        "\n",
        "  while True:\n",
        "    try:\n",
        "       # Let's consume events from the topic. \n",
        "       # poll() function of the consumer allows us to consume a single event\n",
        "       # See docs here https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#confluent_kafka.Consumer.poll\n",
        "       \n",
        "       # Note: If there are no more events to consume, poll()-function will return an empty record.\n",
        "       # In this case, we can stop the execution and break out of the while-loop.\n",
        "      \n",
        "\n",
        "      # The returned event will have a key(), in our case genre, and a value(), which is the play event itself. \n",
        "      # The value() is the row from the csv file, which we have written and is a string representation of an OrderedDict\n",
        "      # Tp transform the string into an OrderedDict, we can use python's eval() function,\n",
        "      # see https://realpython.com/python-eval-function/\n",
        "\n",
        "\n",
        "      # Once you have successfully transformed the value into an OrderedDict, we can start counting e.g. \n",
        "      # how often an artist was played. \n",
        "      # For that, we can use `artist_statistics`, which is a defaultdict, and just increment the dict for the current artist\n",
        "      # artist_statistics[currentArtist?] += 1\n",
        "      \n",
        "      \n",
        "    except KeyboardInterrupt:\n",
        "      break\n",
        "  return artist_statistics\n",
        "\n",
        "# Note: Once you have consumed all messages from a topic, you cannot reconsume them again\n",
        "# Try changing the consumer group string, to reconsume all events again\n",
        "statistics = get_artist_statistics('music-streams', 'group1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KcO2cwmxY5D",
        "outputId": "4b2484e6-80ab-4a5d-a10c-02384b7b8dee"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Let's see which artists were played the most\n",
        "c = Counter(statistics)\n",
        "print(c.most_common(5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('The Beatles', 2451), ('Queen', 2085), ('Coldplay', 1888), ('Michael Jackson', 1511), ('The Rolling Stones', 1496)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQELYCy2zekC"
      },
      "source": [
        "# For the curious minds:\n",
        "\n",
        "\n",
        "*   Write and consume messages with different serialization formats, e.g. JSON or AVRO \n",
        "*   Manage Kafka partitioning and parallelize your consumers, by starting multiple consumers from the same consumer group\n",
        "\n"
      ]
    }
  ]
}